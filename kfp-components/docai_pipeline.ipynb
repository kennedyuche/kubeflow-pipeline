{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "from kfp.components import load_component_from_file\n",
    "import kfp.compiler as compiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create path for data_loader component\n",
    "\n",
    "data_loader_manifest = \"\"\"\\\n",
    "name: Load 10k Files\n",
    "description: Download 10k files to Azure Datalake Storage\n",
    "\n",
    "implementation:\n",
    "  container:\n",
    "    image: docaiacr.azurecr.io/data-loader:v1\n",
    "    command: [ 'python3', '-m', 'data_loader.download_10k_raw' ]\n",
    "\"\"\"\n",
    "\n",
    "data_loader_path = './data_loader_comp.yaml'\n",
    "with open(data_loader_path,'w') as data_loader:\n",
    "    data_loader.write(data_loader_manifest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create path for spark_runner component\n",
    "\n",
    "spark_runner_manifest = \"\"\"\\\n",
    "name: Run Spark Job\n",
    "description: Create docai spark app in the same k8s cluster for Spark-operator\n",
    "implementation:\n",
    "  container:\n",
    "    image: docaiacr.azurecr.io/spark-job:v1  # Update with the image from spark-job dir\n",
    "    command: ['python', 'create_spark_app.py']\n",
    "\"\"\"\n",
    "\n",
    "spark_runner_path = './spark_runner_comp.yaml'\n",
    "with open(spark_runner_path,'w') as spark_runner:\n",
    "    spark_runner.write(spark_runner_manifest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pipeline components\n",
    "\n",
    "create_spark_app_op = load_component_from_file('spark_runner_comp.yaml')\n",
    "\n",
    "create_data_loader_op = load_component_from_file('data_loader_comp.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name='docai data pipeline',\n",
    "    description='Load 10k files to Azure storage, run spark job to pick the data and load back to Azure Storage in delta format'\n",
    ")\n",
    "def docai_data_pipeline():\n",
    "    step1 = create_data_loader_op()\n",
    "\n",
    "    step2 = create_spark_app_op().after(step1)\n",
    "    \n",
    "pipeline_func = docai_data_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile pipeline\n",
    "pipeline_filename = pipeline_func.__name__ + '.yaml'\n",
    "\n",
    "compiler.Compiler().compile(pipeline_func, pipeline_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit and run pipeline\n",
    "arguments = {}\n",
    "\n",
    "client = kfp.Client()\n",
    "experiment = client.create_experiment(\"test\")\n",
    "\n",
    "run_name = pipeline_func.__name__ + ' run'\n",
    "run_result = client.run_pipeline(experiment.id, run_name, pipeline_filename, arguments)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
